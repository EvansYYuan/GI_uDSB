{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfce4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"/home/yyuan/ICB_TCE/\"  # adjust if needed\n",
    "script_dir = os.path.join(work_dir, \"scripts\")\n",
    "\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "iter_dir = os.path.join(work_dir, \"iter_results\")\n",
    "os.makedirs(iter_dir, exist_ok=True)\n",
    "\n",
    "summary_dir = os.path.join(iter_dir, \"summaries\")\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "\n",
    "from vae import *\n",
    "from sde import *\n",
    "from bio_con import *\n",
    "from bio_util import *\n",
    "from training_util import *\n",
    "from joint_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef122fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global matplotlib defaults: Nimbus Roman + dpi=300\n",
    "plt.rcParams[\"font.family\"] = \"Nimbus Roman\"\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"savefig.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2131ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7808bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_brca_t = sc.read_h5ad(os.path.join(work_dir, \"data/brca_t_cell.h5ad\"))\n",
    "\n",
    "# QC and filter highly variable genes\n",
    "sc.pp.filter_cells(adata_brca_t, min_genes = 200)\n",
    "sc.pp.filter_genes(adata_brca_t, min_cells = 3)\n",
    "sc.pp.highly_variable_genes(adata_brca_t, n_top_genes = 3000, subset = True)\n",
    "\n",
    "pre_treatment_mask = adata_brca_t.obs[\"pre_post\"] == \"Pre\"\n",
    "post_treatment_mask = adata_brca_t.obs[\"pre_post\"] == \"Post\"\n",
    "\n",
    "# Convert the sparse matrix to a dense numpy array, then to a PyTorch tensor\n",
    "if hasattr(adata_brca_t.X, \"toarray\"):\n",
    "    expression_data = torch.tensor(adata_brca_t.X.toarray(), dtype=torch.float32)\n",
    "else: # If it's already a dense array\n",
    "    expression_data = torch.tensor(adata_brca_t.X, dtype=torch.float32)\n",
    "\n",
    "input_dim = expression_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18cce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 20\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# KL Annealing Parameters\n",
    "KL_START_EPOCH = 3  # Start KL annealing earlier in light pre-training\n",
    "KL_WARMUP_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler:\n",
    "    def __init__(self, data, device):\n",
    "        self.data = data.to(device)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = torch.randint(0, len(self.data), (batch_size,))\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee10124",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_config_path = os.path.join(work_dir, \"trained_models/full_config.json\")\n",
    "with open(full_config_path, \"r\") as f: \n",
    "    config = json.load(f)\n",
    "config[\"input_dim\"] = expression_data.shape[1]\n",
    "config[\"latent_dim\"] = LATENT_DIM\n",
    "\n",
    "config_abl = config.copy()\n",
    "config_abl[\"lambda_bio\"] = 0.0\n",
    "config_abl[\"lambda_grn\"] = 0.0\n",
    "config_abl[\"lambda_death\"] = 0.0\n",
    "config_abl[\"lambda_birth\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d778cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_seed(seed: int, config: dict, config_abl: dict):\n",
    "    \"\"\"\n",
    "    Run original + ablation training pipeline for a single random seed.\n",
    "\n",
    "    All results are stored under:\n",
    "        iter_results / run_{seed:03d}\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SEED {seed}: starting run (original + ablation)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Set paths for this seed\n",
    "    run_tag = f\"run_{seed:03d}\"\n",
    "    run_dir = os.path.join(iter_dir, run_tag)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    cfg = config.copy()\n",
    "    cfg_abl = config_abl.copy()\n",
    "\n",
    "    # VAE pretraining\n",
    "    print(\"\\nStarting VAE pre-training...\")\n",
    "    set_seed(seed)\n",
    "    dataset = TensorDataset(expression_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    vae_pre = VAE_scRNA(input_dim=cfg[\"input_dim\"], latent_dim=latent_dim).to(device)\n",
    "    optimizer = optim.Adam(vae_pre.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    vae_pre.train()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Calculate beta for KL annealing\n",
    "        if epoch < KL_START_EPOCH:\n",
    "            beta = 0.0\n",
    "        else:\n",
    "            beta = min(1.0, (epoch - KL_START_EPOCH) / KL_WARMUP_EPOCHS)\n",
    "\n",
    "        for (batch_features,) in dataloader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_x, mu, log_var = vae(batch_features)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = elbo_loss(batch_features, recon_x, mu, log_var, beta=beta)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "    print(\"\\nVAE pre-training complete.\")\n",
    "\n",
    "    vae_pretrain_path = os.path.join(run_dir, f\"vae_pretrain_{run_tag}.pth\")\n",
    "    torch.save(vae_pre.state_dict(), vae_pretrain_path)\n",
    "\n",
    "    # Latent embeddings for bridge\n",
    "    latent_mu = compute_latent_embeddings(vae_pre, expression_data, device=device)\n",
    "    latent_embeddings = latent_mu.numpy()\n",
    "\n",
    "    pre_embeddings = torch.tensor(\n",
    "        latent_embeddings[pre_treatment_mask], dtype=torch.float32\n",
    "    ).to(device)\n",
    "    post_embeddings = torch.tensor(\n",
    "        latent_embeddings[post_treatment_mask], dtype=torch.float32\n",
    "    ).to(device)\n",
    "\n",
    "    p_sampler = DataSampler(pre_embeddings, device=device)\n",
    "    q_sampler = DataSampler(post_embeddings, device=device)\n",
    "\n",
    "    # Joint training (original)\n",
    "    print(\"\\nStarting joint training (original)...\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Load pretrained VAE weights\n",
    "    vae = load_trained_vae(\n",
    "        model_path=vae_pretrain_path,\n",
    "        input_dim=input_dim,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    dyn = VESDE(cfg, p_sampler, q_sampler)\n",
    "    ts = torch.linspace(cfg[\"t0\"], cfg[\"T\"], cfg[\"interval\"]).to(device)\n",
    "\n",
    "    net_f = MLP(input_dim=cfg[\"data_dim\"][0], output_dim=cfg[\"data_dim\"][0]).to(device)\n",
    "    net_b = MLP(input_dim=cfg[\"data_dim\"][0], output_dim=cfg[\"data_dim\"][0]).to(device)\n",
    "\n",
    "    z_f = SchrodingerBridgePolicy(cfg, \"forward\", dyn, net_f)\n",
    "    z_b = SchrodingerBridgePolicy(cfg, \"backward\", dyn, net_b)\n",
    "\n",
    "    optimizer_f = torch.optim.Adam(z_f.parameters(), lr=cfg[\"lr\"])\n",
    "    optimizer_b = torch.optim.Adam(z_b.parameters(), lr=cfg[\"lr\"])\n",
    "    optimizer_vae = torch.optim.Adam(vae.parameters(), lr=cfg.get(\"lr_vae\", 1e-4))\n",
    "\n",
    "    vae_decoder = lambda z: vae.decoder_output(vae.decoder(z))\n",
    "\n",
    "    training_history = run_joint_training_loop(\n",
    "        config=cfg,\n",
    "        dyn=dyn,\n",
    "        ts=ts,\n",
    "        vae=vae,\n",
    "        vae_decoder=vae_decoder,\n",
    "        z_f=z_f,\n",
    "        z_b=z_b,\n",
    "        optimizer_f=optimizer_f,\n",
    "        optimizer_b=optimizer_b,\n",
    "        optimizer_vae=optimizer_vae,\n",
    "        expression_data=expression_data,          # computed once outside\n",
    "        grn_data=grn_data,\n",
    "        death_gene_indices=death_gene_indices,\n",
    "        birth_gene_indices=birth_gene_indices,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Save original models + history for this run\n",
    "    torch.save(vae.state_dict(), os.path.join(run_dir, f\"vae_original_{run_tag}.pth\"))\n",
    "    torch.save(z_f.state_dict(), os.path.join(run_dir, f\"z_f_original_{run_tag}.pth\"))\n",
    "    torch.save(z_b.state_dict(), os.path.join(run_dir, f\"z_b_original_{run_tag}.pth\"))\n",
    "\n",
    "    hist_path = os.path.join(run_dir, f\"training_history_original_{run_tag}.json\")\n",
    "    with open(hist_path, \"w\") as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    print(f\"[Seed {seed}] Joint training complete; history saved to {hist_path}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Ablation (no biology constraints) â€“ fresh VAE\n",
    "    # -------------------------\n",
    "    vae_abl = VAE_scRNA(\n",
    "        input_dim=config_abl[\"input_dim\"], latent_dim=cfg_abl[\"data_dim\"][0]\n",
    "    ).to(device)\n",
    "\n",
    "    z_f_abl = SchrodingerBridgePolicy(\n",
    "        cfg_abl,\n",
    "        \"forward\",\n",
    "        dyn,\n",
    "        MLP(cfg_abl[\"data_dim\"][0], cfg_abl[\"data_dim\"][0]).to(device),\n",
    "    ).to(device)\n",
    "\n",
    "    z_b_abl = SchrodingerBridgePolicy(\n",
    "        cfg_abl,\n",
    "        \"backward\",\n",
    "        dyn,\n",
    "        MLP(cfg_abl[\"data_dim\"][0], cfg_abl[\"data_dim\"][0]).to(device),\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer_vae_abl = torch.optim.Adam(\n",
    "        vae_abl.parameters(), lr=cfg_abl[\"lr\"], weight_decay=1e-4\n",
    "    )\n",
    "    optimizer_f_abl = torch.optim.Adam(\n",
    "        z_f_abl.parameters(), lr=cfg_abl[\"lr\"], weight_decay=1e-4\n",
    "    )\n",
    "    optimizer_b_abl = torch.optim.Adam(\n",
    "        z_b_abl.parameters(), lr=cfg_abl[\"lr\"], weight_decay=1e-4\n",
    "    )\n",
    "    vae_decoder_abl = lambda z: vae_abl.decoder_output(vae_abl.decoder(z))\n",
    "\n",
    "    ablation_history = run_joint_training_loop(\n",
    "        config=cfg_abl,\n",
    "        dyn=dyn,\n",
    "        ts=ts,\n",
    "        vae=vae_abl,\n",
    "        vae_decoder=vae_decoder_abl,\n",
    "        z_f=z_f_abl,\n",
    "        z_b=z_b_abl,\n",
    "        optimizer_f=optimizer_f_abl,\n",
    "        optimizer_b=optimizer_b_abl,\n",
    "        optimizer_vae=optimizer_vae_abl,\n",
    "        expression_data=expression_data,\n",
    "        grn_data=grn_data,\n",
    "        death_gene_indices=death_gene_indices,\n",
    "        birth_gene_indices=birth_gene_indices,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Save ablation models + history\n",
    "    torch.save(vae_abl.state_dict(), os.path.join(run_dir, f\"vae_ablation_{run_tag}.pth\"))\n",
    "    torch.save(z_f_abl.state_dict(), os.path.join(run_dir, f\"z_f_ablation_{run_tag}.pth\"))\n",
    "    torch.save(z_b_abl.state_dict(), os.path.join(run_dir, f\"z_b_ablation_{run_tag}.pth\"))\n",
    "\n",
    "    ablation_hist_path = os.path.join(run_dir, f\"training_history_ablation_{run_tag}.json\")\n",
    "    with open(ablation_hist_path, \"w\") as f:\n",
    "        json.dump(ablation_history, f, indent=2)\n",
    "\n",
    "    # Drift genes (original + ablation)\n",
    "    drift_fwd_orig, drift_bwd_orig = compute_drift_genes_for_models(\n",
    "        vae=vae,\n",
    "        z_f=z_f,\n",
    "        z_b=z_b,\n",
    "        adata=adata_brca_t,   # use global adata\n",
    "        config=cfg,\n",
    "        device=device,\n",
    "    )\n",
    "    drift_fwd_abl, drift_bwd_abl = compute_drift_genes_for_models(\n",
    "        vae=vae_abl,\n",
    "        z_f=z_f_abl,\n",
    "        z_b=z_b_abl,\n",
    "        adata=adata_brca_t,\n",
    "        config=cfg_abl,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    drift_fwd_orig.to_csv(os.path.join(run_dir, \"drift_genes_forward_original.csv\"), index=False)\n",
    "    drift_bwd_orig.to_csv(os.path.join(run_dir, \"drift_genes_backward_original.csv\"), index=False)\n",
    "    drift_fwd_abl.to_csv(os.path.join(run_dir, \"drift_genes_forward_ablation.csv\"), index=False)\n",
    "    drift_bwd_abl.to_csv(os.path.join(run_dir, \"drift_genes_backward_ablation.csv\"), index=False)\n",
    "\n",
    "    print(f\"[Seed {seed}] Drift genes saved in {run_dir}\")\n",
    "    print(f\"[Seed {seed}] run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over 20 seeds and run full pipeline\n",
    "NUM_SEEDS = 20\n",
    "SEEDS = list(range(1, NUM_SEEDS + 1))\n",
    "\n",
    "for s in SEEDS:\n",
    "    run_single_seed(seed=s, config=config, config_abl=config_abl)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
