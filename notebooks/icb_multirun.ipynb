{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bfce4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebbf0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"/home/yyuan/ICB_TCE/\"  # adjust if needed\n",
    "script_dir = os.path.join(work_dir, \"scripts\")\n",
    "\n",
    "if script_dir not in sys.path:\n",
    "    sys.path.append(script_dir)\n",
    "\n",
    "iter_dir = os.path.join(work_dir, \"iter_results\")\n",
    "os.makedirs(iter_dir, exist_ok=True)\n",
    "\n",
    "summary_dir = os.path.join(iter_dir, \"summaries\")\n",
    "os.makedirs(summary_dir, exist_ok=True)\n",
    "\n",
    "from vae import *\n",
    "from sde import *\n",
    "from bio_con import *\n",
    "from bio_util import *\n",
    "from training_util import *\n",
    "from joint_train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef122fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global matplotlib defaults: Nimbus Roman + dpi=300\n",
    "plt.rcParams[\"font.family\"] = \"Nimbus Roman\"\n",
    "plt.rcParams[\"figure.dpi\"] = 300\n",
    "plt.rcParams[\"savefig.dpi\"] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5460fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2131ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7808bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_brca_t = sc.read_h5ad(os.path.join(work_dir, \"data/brca_t_cell.h5ad\"))\n",
    "\n",
    "# QC and filter highly variable genes\n",
    "sc.pp.filter_cells(adata_brca_t, min_genes = 200)\n",
    "sc.pp.filter_genes(adata_brca_t, min_cells = 3)\n",
    "sc.pp.highly_variable_genes(adata_brca_t, n_top_genes = 3000, subset = True)\n",
    "\n",
    "pre_treatment_mask = adata_brca_t.obs[\"pre_post\"] == \"Pre\"\n",
    "post_treatment_mask = adata_brca_t.obs[\"pre_post\"] == \"Post\"\n",
    "\n",
    "# Convert the sparse matrix to a dense numpy array, then to a PyTorch tensor\n",
    "if hasattr(adata_brca_t.X, \"toarray\"):\n",
    "    expression_data = torch.tensor(adata_brca_t.X.toarray(), dtype=torch.float32)\n",
    "else: # If it's already a dense array\n",
    "    expression_data = torch.tensor(adata_brca_t.X, dtype=torch.float32)\n",
    "\n",
    "input_dim = expression_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a18cce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 20\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# KL Annealing Parameters\n",
    "KL_START_EPOCH = 3  # Start KL annealing earlier in light pre-training\n",
    "KL_WARMUP_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aeb5a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSampler:\n",
    "    def __init__(self, data, device):\n",
    "        self.data = data.to(device)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = torch.randint(0, len(self.data), (batch_size,))\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee10124",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_config_path = os.path.join(work_dir, \"trained_models/full_config.json\")\n",
    "with open(full_config_path, \"r\") as f: \n",
    "    config = json.load(f)\n",
    "\n",
    "config_abl = config.copy()\n",
    "config_abl[\"lambda_bio\"] = 0.0\n",
    "config_abl[\"lambda_grn\"] = 0.0\n",
    "config_abl[\"lambda_death\"] = 0.0\n",
    "config_abl[\"lambda_birth\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ffbfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Cell Death' Gene Sets:\n",
      "Loaded 161 genes from HALLMARK_APOPTOSIS\n",
      "Loaded 200 genes from HALLMARK_P53_PATHWAY\n",
      "Loaded 49 genes from HALLMARK_REACTIVE_OXYGEN_SPECIES_PATHWAY\n",
      "Loaded 113 genes from HALLMARK_UNFOLDED_PROTEIN_RESPONSE\n",
      "\n",
      "'Cell Birth' Gene Sets:\n",
      "Loaded 200 genes from HALLMARK_E2F_TARGETS\n",
      "Loaded 200 genes from HALLMARK_G2M_CHECKPOINT\n",
      "Loaded 200 genes from HALLMARK_MYC_TARGETS_V1\n",
      "Loaded 58 genes from HALLMARK_MYC_TARGETS_V2\n",
      "\n",
      "Found 109 matching cell death genes in the HVG set.\n",
      "Found 32 matching cell birth genes in the HVG set.\n"
     ]
    }
   ],
   "source": [
    "# Prepare cell fate gene sets\n",
    "cell_death_files = [\n",
    "    os.path.join(work_dir, \"GO_geneset/HALLMARK_APOPTOSIS.v2025.1.Hs.json\"),\n",
    "    os.path.join(work_dir, \"GO_geneset/HALLMARK_P53_PATHWAY.v2025.1.Hs.json\"),\n",
    "    os.path.join(work_dir, \"GO_geneset/HALLMARK_REACTIVE_OXYGEN_SPECIES_PATHWAY.v2025.1.Hs.json\"),\n",
    "    os.path.join(work_dir, \"GO_geneset/HALLMARK_UNFOLDED_PROTEIN_RESPONSE.v2025.1.Hs.json\"),\n",
    "]\n",
    "\n",
    "cell_birth_files = [\n",
    "    os.path.join(work_dir, \"GO_geneset/HALLMARK_E2F_TARGETS.v2025.1.Hs.json\"),\n",
    "    os.path.join(work_dir, \"GO_geneset/HALLMARK_G2M_CHECKPOINT.v2025.1.Hs.json\"),\n",
    "    os.path.join(work_dir, \"GO_geneset/HALLMARK_MYC_TARGETS_V1.v2025.1.Hs.json\"),\n",
    "    os.path.join(work_dir, \"GO_geneset/HALLMARK_MYC_TARGETS_V2.v2025.1.Hs.json\"),\n",
    "]\n",
    "\n",
    "all_death_genes = set()\n",
    "all_birth_genes = set()\n",
    "\n",
    "print(\"'Cell Death' Gene Sets:\")\n",
    "for file_path in cell_death_files:\n",
    "    name, genes = load_gene_set_from_json(file_path)\n",
    "    if genes:\n",
    "        all_death_genes.update(genes)\n",
    "\n",
    "print(\"\\n'Cell Birth' Gene Sets:\")\n",
    "for file_path in cell_birth_files:\n",
    "    name, genes = load_gene_set_from_json(file_path)\n",
    "    if genes:\n",
    "        all_birth_genes.update(genes)\n",
    "\n",
    "all_death_genes_list = sorted(list(all_death_genes))\n",
    "all_birth_genes_list = sorted(list(all_birth_genes))\n",
    "\n",
    "# Constraint preparation using HVG subset\n",
    "hvg_names_list = adata_brca_t.var['feature_name'].tolist()\n",
    "\n",
    "# Death genes & their indices within the HVG list\n",
    "death_genes_in_hvg = [g for g in all_death_genes_list if g in hvg_names_list]\n",
    "death_gene_indices = [hvg_names_list.index(g) for g in death_genes_in_hvg]\n",
    "print(f\"\\nFound {len(death_gene_indices)} matching cell death genes in the HVG set.\")\n",
    "\n",
    "# Birth genes & their indices within the HVG list\n",
    "birth_genes_in_hvg = [g for g in all_birth_genes_list if g in hvg_names_list]\n",
    "birth_gene_indices = [hvg_names_list.index(g) for g in birth_genes_in_hvg]\n",
    "print(f\"Found {len(birth_gene_indices)} matching cell birth genes in the HVG set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f62ce19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 123 GRN edges overlapping with prior knowledge.\n",
      "Loaded 53 GRN rules where both TF and target are in the HVG set.\n",
      "GRN constraints (HVG-filtered): 53 edges\n"
     ]
    }
   ],
   "source": [
    "# Prepare GRN constraints\n",
    "adj_file = os.path.join(work_dir, \"data/brca_t_cell_adj.csv\")\n",
    "prior_edges_file = os.path.join(work_dir, \"data/TCE_prior_edges.csv\")\n",
    "\n",
    "grn_df_hvg, grn_data = build_grn_with_prior(\n",
    "    adj_path=adj_file,\n",
    "    prior_edges_path=prior_edges_file,\n",
    "    hvg_names_list=hvg_names_list,\n",
    ")\n",
    "\n",
    "print(f\"GRN constraints (HVG-filtered): {grn_df_hvg.shape[0]} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2d778cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_seed(seed: int, config: dict, config_abl: dict):\n",
    "    \"\"\"\n",
    "    Run original + ablation training pipeline for a single random seed.\n",
    "\n",
    "    All results are stored under:\n",
    "        iter_results / run_{seed:03d}\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"[Seed {seed}] Starting running whole pipeline...\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Set paths for this seed\n",
    "    run_tag = f\"run_{seed:03d}\"\n",
    "    run_dir = os.path.join(iter_dir, run_tag)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    cfg = config.copy()\n",
    "    cfg_abl = config_abl.copy()\n",
    "\n",
    "    # -------------------------    \n",
    "    # VAE pretraining\n",
    "    # -------------------------\n",
    "    print(f\"\\n[Seed {seed}] Starting VAE pre-training...\\n\")\n",
    "    set_seed(seed)\n",
    "    dataset = TensorDataset(expression_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    vae_pre = VAE_scRNA(input_dim=input_dim, latent_dim=LATENT_DIM).to(device)\n",
    "    optimizer = optim.Adam(vae_pre.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "    vae_pre.train()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Calculate beta for KL annealing\n",
    "        if epoch < KL_START_EPOCH:\n",
    "            beta = 0.0\n",
    "        else:\n",
    "            beta = min(1.0, (epoch - KL_START_EPOCH) / KL_WARMUP_EPOCHS)\n",
    "\n",
    "        for (batch_features,) in dataloader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            recon_x, mu, log_var = vae_pre(batch_features)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = elbo_loss(batch_features, recon_x, mu, log_var, beta=beta)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        print(f\"    Epoch [{epoch+1:02d}/{NUM_EPOCHS}], Beta: {beta:.3f}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "    print(f\"\\n[Seed {seed}] VAE pre-training complete.\")\n",
    "\n",
    "    vae_pretrain_path = os.path.join(run_dir, f\"vae_pretrain_{run_tag}.pth\")\n",
    "    torch.save(vae_pre.state_dict(), vae_pretrain_path)\n",
    "\n",
    "    # Latent embeddings for bridge\n",
    "    latent_mu = compute_latent_embeddings(vae_pre, expression_data, device=device)\n",
    "    latent_embeddings = latent_mu.numpy()\n",
    "\n",
    "    pre_embeddings = torch.tensor(\n",
    "        latent_embeddings[pre_treatment_mask], dtype=torch.float32\n",
    "    ).to(device)\n",
    "    post_embeddings = torch.tensor(\n",
    "        latent_embeddings[post_treatment_mask], dtype=torch.float32\n",
    "    ).to(device)\n",
    "\n",
    "    p_sampler = DataSampler(pre_embeddings, device=device)\n",
    "    q_sampler = DataSampler(post_embeddings, device=device)\n",
    "\n",
    "    # -------------------------\n",
    "    # Joint training (original)\n",
    "    # -------------------------    \n",
    "    print(f\"\\n[Seed {seed}] Starting joint training (original)...\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Load pretrained VAE weights\n",
    "    vae = load_trained_vae(\n",
    "        model_path=vae_pretrain_path,\n",
    "        input_dim=input_dim,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    dyn = VESDE(cfg, p_sampler, q_sampler)\n",
    "    ts = torch.linspace(cfg[\"t0\"], cfg[\"T\"], cfg[\"interval\"]).to(device)\n",
    "\n",
    "    net_f = MLP(input_dim=cfg[\"data_dim\"][0], output_dim=cfg[\"data_dim\"][0]).to(device)\n",
    "    net_b = MLP(input_dim=cfg[\"data_dim\"][0], output_dim=cfg[\"data_dim\"][0]).to(device)\n",
    "\n",
    "    z_f = SchrodingerBridgePolicy(cfg, \"forward\", dyn, net_f)\n",
    "    z_b = SchrodingerBridgePolicy(cfg, \"backward\", dyn, net_b)\n",
    "\n",
    "    optimizer_f = torch.optim.Adam(z_f.parameters(), lr=cfg[\"lr\"])\n",
    "    optimizer_b = torch.optim.Adam(z_b.parameters(), lr=cfg[\"lr\"])\n",
    "    optimizer_vae = torch.optim.Adam(vae.parameters(), lr=cfg.get(\"lr_vae\", 1e-4))\n",
    "\n",
    "    vae_decoder = lambda z: vae.decoder_output(vae.decoder(z))\n",
    "\n",
    "    training_history = run_joint_training_loop(\n",
    "        config=cfg,\n",
    "        dyn=dyn,\n",
    "        ts=ts,\n",
    "        vae=vae,\n",
    "        vae_decoder=vae_decoder,\n",
    "        z_f=z_f,\n",
    "        z_b=z_b,\n",
    "        optimizer_f=optimizer_f,\n",
    "        optimizer_b=optimizer_b,\n",
    "        optimizer_vae=optimizer_vae,\n",
    "        expression_data=expression_data,\n",
    "        grn_data=grn_data,\n",
    "        death_gene_indices=death_gene_indices,\n",
    "        birth_gene_indices=birth_gene_indices,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Save original models + history for this run\n",
    "    torch.save(vae.state_dict(), os.path.join(run_dir, f\"vae_original_{run_tag}.pth\"))\n",
    "    torch.save(z_f.state_dict(), os.path.join(run_dir, f\"z_f_original_{run_tag}.pth\"))\n",
    "    torch.save(z_b.state_dict(), os.path.join(run_dir, f\"z_b_original_{run_tag}.pth\"))\n",
    "\n",
    "    hist_path = os.path.join(run_dir, f\"training_history_original_{run_tag}.json\")\n",
    "    with open(hist_path, \"w\") as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    print(f\"\\n[Seed {seed}] Joint training (original) complete; models and history saved.\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Ablation (no biology constraints)\n",
    "    # -------------------------\n",
    "    set_seed(seed)\n",
    "    print(f\"\\n[Seed {seed}] Starting joint training (ablation)...\")\n",
    "\n",
    "    vae_abl = load_trained_vae(\n",
    "        model_path=vae_pretrain_path,\n",
    "        input_dim=input_dim,\n",
    "        latent_dim=LATENT_DIM,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    z_f_abl = SchrodingerBridgePolicy(\n",
    "        cfg_abl,\n",
    "        \"forward\",\n",
    "        dyn,\n",
    "        MLP(cfg_abl[\"data_dim\"][0], cfg_abl[\"data_dim\"][0]).to(device),\n",
    "    ).to(device)\n",
    "\n",
    "    z_b_abl = SchrodingerBridgePolicy(\n",
    "        cfg_abl,\n",
    "        \"backward\",\n",
    "        dyn,\n",
    "        MLP(cfg_abl[\"data_dim\"][0], cfg_abl[\"data_dim\"][0]).to(device),\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer_vae_abl = torch.optim.Adam(\n",
    "        vae_abl.parameters(), lr=cfg_abl[\"lr\"], weight_decay=1e-4\n",
    "    )\n",
    "    optimizer_f_abl = torch.optim.Adam(\n",
    "        z_f_abl.parameters(), lr=cfg_abl[\"lr\"], weight_decay=1e-4\n",
    "    )\n",
    "    optimizer_b_abl = torch.optim.Adam(\n",
    "        z_b_abl.parameters(), lr=cfg_abl[\"lr\"], weight_decay=1e-4\n",
    "    )\n",
    "    vae_decoder_abl = lambda z: vae_abl.decoder_output(vae_abl.decoder(z))\n",
    "\n",
    "    ablation_history = run_joint_training_loop(\n",
    "        config=cfg_abl,\n",
    "        dyn=dyn,\n",
    "        ts=ts,\n",
    "        vae=vae_abl,\n",
    "        vae_decoder=vae_decoder_abl,\n",
    "        z_f=z_f_abl,\n",
    "        z_b=z_b_abl,\n",
    "        optimizer_f=optimizer_f_abl,\n",
    "        optimizer_b=optimizer_b_abl,\n",
    "        optimizer_vae=optimizer_vae_abl,\n",
    "        expression_data=expression_data,\n",
    "        grn_data=grn_data,\n",
    "        death_gene_indices=death_gene_indices,\n",
    "        birth_gene_indices=birth_gene_indices,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Save ablation models + history\n",
    "    torch.save(vae_abl.state_dict(), os.path.join(run_dir, f\"vae_ablation_{run_tag}.pth\"))\n",
    "    torch.save(z_f_abl.state_dict(), os.path.join(run_dir, f\"z_f_ablation_{run_tag}.pth\"))\n",
    "    torch.save(z_b_abl.state_dict(), os.path.join(run_dir, f\"z_b_ablation_{run_tag}.pth\"))\n",
    "\n",
    "    ablation_hist_path = os.path.join(run_dir, f\"training_history_ablation_{run_tag}.json\")\n",
    "    with open(ablation_hist_path, \"w\") as f:\n",
    "        json.dump(ablation_history, f, indent=2)\n",
    "    print(f\"\\n[Seed {seed}] Joint training (ablation) complete; models and history saved.\")\n",
    "\n",
    "    # Drift genes (original + ablation)\n",
    "    pre_cells_expr = expression_data[pre_treatment_mask].to(device)\n",
    "    post_cells_expr = expression_data[post_treatment_mask].to(device)\n",
    "    gene_symbols = hvg_names_list  # HVG feature names prepared above\n",
    "\n",
    "    drift_fwd_orig, drift_bwd_orig = compute_drift_tables(\n",
    "        vae=vae,\n",
    "        z_f=z_f,\n",
    "        z_b=z_b,\n",
    "        pre_cells_expr=pre_cells_expr,\n",
    "        post_cells_expr=post_cells_expr,\n",
    "        gene_symbols=gene_symbols,\n",
    "    )\n",
    "    drift_fwd_abl, drift_bwd_abl = compute_drift_tables(\n",
    "        vae=vae_abl,\n",
    "        z_f=z_f_abl,\n",
    "        z_b=z_b_abl,\n",
    "        pre_cells_expr=pre_cells_expr,\n",
    "        post_cells_expr=post_cells_expr,\n",
    "        gene_symbols=gene_symbols,\n",
    "    )\n",
    "\n",
    "    drift_fwd_orig.to_csv(os.path.join(run_dir, \"drift_genes_forward_original.csv\"), index=False)\n",
    "    drift_bwd_orig.to_csv(os.path.join(run_dir, \"drift_genes_backward_original.csv\"), index=False)\n",
    "    drift_fwd_abl.to_csv(os.path.join(run_dir, \"drift_genes_forward_ablation.csv\"), index=False)\n",
    "    drift_bwd_abl.to_csv(os.path.join(run_dir, \"drift_genes_backward_ablation.csv\"), index=False)\n",
    "\n",
    "    print(f\"\\n[Seed {seed}] Drift genes saved in {run_dir}\")\n",
    "    print(f\"\\n[Seed {seed}] run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d94a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "[Seed 1] Starting running whole pipeline...\n",
      "================================================================================\n",
      "\n",
      "[Seed 1] Starting VAE pre-training...\n",
      "\n",
      "    Epoch [01/20], Beta: 0.000, Average Loss: 542.4795\n",
      "    Epoch [02/20], Beta: 0.000, Average Loss: 305.0493\n",
      "    Epoch [03/20], Beta: 0.000, Average Loss: 270.9695\n",
      "    Epoch [04/20], Beta: 0.000, Average Loss: 252.0539\n",
      "    Epoch [05/20], Beta: 0.100, Average Loss: 246.8968\n",
      "    Epoch [06/20], Beta: 0.200, Average Loss: 245.4504\n",
      "    Epoch [07/20], Beta: 0.300, Average Loss: 245.5577\n",
      "    Epoch [08/20], Beta: 0.400, Average Loss: 246.4791\n",
      "    Epoch [09/20], Beta: 0.500, Average Loss: 247.6631\n",
      "    Epoch [10/20], Beta: 0.600, Average Loss: 248.8001\n",
      "    Epoch [11/20], Beta: 0.700, Average Loss: 249.9610\n",
      "    Epoch [12/20], Beta: 0.800, Average Loss: 251.1498\n",
      "    Epoch [13/20], Beta: 0.900, Average Loss: 252.3078\n",
      "    Epoch [14/20], Beta: 1.000, Average Loss: 253.8883\n",
      "    Epoch [15/20], Beta: 1.000, Average Loss: 253.4081\n",
      "    Epoch [16/20], Beta: 1.000, Average Loss: 253.0522\n",
      "    Epoch [17/20], Beta: 1.000, Average Loss: 252.6847\n",
      "    Epoch [18/20], Beta: 1.000, Average Loss: 252.2809\n",
      "    Epoch [19/20], Beta: 1.000, Average Loss: 251.9322\n",
      "    Epoch [20/20], Beta: 1.000, Average Loss: 251.7606\n",
      "\n",
      "[Seed 1] VAE pre-training complete.\n",
      "\n",
      "[Seed 1] Starting joint training (original)...\n",
      "\n",
      "======================================================================\n",
      "STAGE 1/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=259.7630, vae=3.4529, dsb=242.8202, bio=26.9798\n",
      "    Epoch  2/10: total=72.5722, vae=1.0875, dsb=71.4507, bio=0.0681\n",
      "    Epoch  3/10: total=64.0931, vae=1.0815, dsb=62.9973, bio=0.0285\n",
      "    Epoch  4/10: total=60.1881, vae=1.0801, dsb=59.0998, bio=0.0163\n",
      "    Epoch  5/10: total=57.5891, vae=1.0786, dsb=56.5051, bio=0.0109\n",
      "    Epoch  6/10: total=55.6571, vae=1.0780, dsb=54.5752, bio=0.0078\n",
      "    Epoch  7/10: total=54.6015, vae=1.0765, dsb=53.5220, bio=0.0061\n",
      "    Epoch  8/10: total=53.3342, vae=1.0776, dsb=52.2540, bio=0.0051\n",
      "    Epoch  9/10: total=52.6424, vae=1.0774, dsb=51.5629, bio=0.0043\n",
      "    Epoch 10/10: total=52.0524, vae=1.0767, dsb=50.9736, bio=0.0042\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=3.0455, vae=1.0776, dsb=1.9628, bio=0.0100\n",
      "    Epoch  2/10: total=2.5861, vae=1.0776, dsb=1.5065, bio=0.0039\n",
      "    Epoch  3/10: total=2.4712, vae=1.0763, dsb=1.3934, bio=0.0029\n",
      "    Epoch  4/10: total=2.4491, vae=1.0773, dsb=1.3705, bio=0.0025\n",
      "    Epoch  5/10: total=2.3828, vae=1.0771, dsb=1.3045, bio=0.0023\n",
      "    Epoch  6/10: total=2.3730, vae=1.0750, dsb=1.2969, bio=0.0021\n",
      "    Epoch  7/10: total=2.2927, vae=1.0766, dsb=1.2150, bio=0.0020\n",
      "    Epoch  8/10: total=2.2827, vae=1.0753, dsb=1.2064, bio=0.0019\n",
      "    Epoch  9/10: total=2.2981, vae=1.0756, dsb=1.2213, bio=0.0024\n",
      "    Epoch 10/10: total=2.2706, vae=1.0750, dsb=1.1946, bio=0.0019\n",
      "\n",
      "======================================================================\n",
      "STAGE 2/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=44.3065, vae=1.0751, dsb=43.2303, bio=0.0022\n",
      "    Epoch  2/10: total=39.8232, vae=1.0759, dsb=38.7463, bio=0.0018\n",
      "    Epoch  3/10: total=38.7179, vae=1.0749, dsb=37.6423, bio=0.0016\n",
      "    Epoch  4/10: total=38.2431, vae=1.0754, dsb=37.1668, bio=0.0018\n",
      "    Epoch  5/10: total=37.9049, vae=1.0748, dsb=36.8293, bio=0.0017\n",
      "    Epoch  6/10: total=37.6277, vae=1.0740, dsb=36.5528, bio=0.0017\n",
      "    Epoch  7/10: total=37.5010, vae=1.0741, dsb=36.4261, bio=0.0017\n",
      "    Epoch  8/10: total=37.3988, vae=1.0750, dsb=36.3230, bio=0.0016\n",
      "    Epoch  9/10: total=36.9402, vae=1.0738, dsb=35.8656, bio=0.0016\n",
      "    Epoch 10/10: total=36.8314, vae=1.0740, dsb=35.7566, bio=0.0016\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=2.0536, vae=1.0751, dsb=0.9754, bio=0.0063\n",
      "    Epoch  2/10: total=1.9601, vae=1.0753, dsb=0.8839, bio=0.0017\n",
      "    Epoch  3/10: total=1.9155, vae=1.0746, dsb=0.8401, bio=0.0015\n",
      "    Epoch  4/10: total=1.9271, vae=1.0750, dsb=0.8515, bio=0.0012\n",
      "    Epoch  5/10: total=1.8849, vae=1.0741, dsb=0.8100, bio=0.0017\n",
      "    Epoch  6/10: total=1.8792, vae=1.0748, dsb=0.8038, bio=0.0014\n",
      "    Epoch  7/10: total=1.8701, vae=1.0745, dsb=0.7949, bio=0.0014\n",
      "    Epoch  8/10: total=1.8880, vae=1.0748, dsb=0.8125, bio=0.0013\n",
      "    Epoch  9/10: total=1.8351, vae=1.0746, dsb=0.7598, bio=0.0013\n",
      "    Epoch 10/10: total=1.8356, vae=1.0744, dsb=0.7605, bio=0.0013\n",
      "\n",
      "======================================================================\n",
      "STAGE 3/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=36.7132, vae=1.0744, dsb=35.6380, bio=0.0015\n",
      "    Epoch  2/10: total=35.0931, vae=1.0734, dsb=34.0190, bio=0.0013\n",
      "    Epoch  3/10: total=34.6547, vae=1.0739, dsb=33.5801, bio=0.0013\n",
      "    Epoch  4/10: total=34.4785, vae=1.0734, dsb=33.4044, bio=0.0012\n",
      "    Epoch  5/10: total=34.4257, vae=1.0734, dsb=33.3517, bio=0.0012\n",
      "    Epoch  6/10: total=34.2306, vae=1.0727, dsb=33.1573, bio=0.0012\n",
      "    Epoch  7/10: total=34.1523, vae=1.0731, dsb=33.0785, bio=0.0013\n",
      "    Epoch  8/10: total=34.0169, vae=1.0731, dsb=32.9433, bio=0.0011\n",
      "    Epoch  9/10: total=34.0264, vae=1.0726, dsb=32.9531, bio=0.0013\n",
      "    Epoch 10/10: total=33.7371, vae=1.0723, dsb=32.6642, bio=0.0011\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=1.7990, vae=1.0738, dsb=0.6872, bio=0.0760\n",
      "    Epoch  2/10: total=1.6974, vae=1.0736, dsb=0.6231, bio=0.0014\n",
      "    Epoch  3/10: total=1.6897, vae=1.0734, dsb=0.6156, bio=0.0013\n",
      "    Epoch  4/10: total=1.6564, vae=1.0739, dsb=0.5820, bio=0.0010\n",
      "    Epoch  5/10: total=1.6670, vae=1.0744, dsb=0.5919, bio=0.0014\n",
      "    Epoch  6/10: total=1.6679, vae=1.0728, dsb=0.5946, bio=0.0012\n",
      "    Epoch  7/10: total=1.6657, vae=1.0728, dsb=0.5924, bio=0.0011\n",
      "    Epoch  8/10: total=1.6597, vae=1.0729, dsb=0.5862, bio=0.0011\n",
      "    Epoch  9/10: total=1.6608, vae=1.0732, dsb=0.5870, bio=0.0011\n",
      "    Epoch 10/10: total=1.6490, vae=1.0727, dsb=0.5758, bio=0.0009\n",
      "\n",
      "======================================================================\n",
      "STAGE 4/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=29.8832, vae=1.0740, dsb=28.8084, bio=0.0015\n",
      "    Epoch  2/10: total=29.1559, vae=1.0731, dsb=28.0823, bio=0.0010\n",
      "    Epoch  3/10: total=28.9945, vae=1.0738, dsb=27.9202, bio=0.0010\n",
      "    Epoch  4/10: total=28.9202, vae=1.0726, dsb=27.8471, bio=0.0011\n",
      "    Epoch  5/10: total=28.9135, vae=1.0717, dsb=27.8412, bio=0.0010\n",
      "    Epoch  6/10: total=29.0164, vae=1.0729, dsb=27.9430, bio=0.0010\n",
      "    Epoch  7/10: total=28.8938, vae=1.0724, dsb=27.8209, bio=0.0010\n",
      "    Epoch  8/10: total=28.7337, vae=1.0732, dsb=27.6602, bio=0.0004\n",
      "    Epoch  9/10: total=28.8158, vae=1.0719, dsb=27.7435, bio=0.0008\n",
      "    Epoch 10/10: total=28.9394, vae=1.0720, dsb=27.8669, bio=0.0011\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=1.6588, vae=1.0740, dsb=0.5143, bio=0.1410\n",
      "    Epoch  2/10: total=1.5692, vae=1.0739, dsb=0.4946, bio=0.0014\n",
      "    Epoch  3/10: total=1.5573, vae=1.0737, dsb=0.4830, bio=0.0011\n",
      "    Epoch  4/10: total=1.5541, vae=1.0733, dsb=0.4803, bio=0.0011\n",
      "    Epoch  5/10: total=1.5478, vae=1.0734, dsb=0.4739, bio=0.0011\n",
      "    Epoch  6/10: total=1.5443, vae=1.0727, dsb=0.4711, bio=0.0010\n",
      "    Epoch  7/10: total=1.5417, vae=1.0734, dsb=0.4677, bio=0.0010\n",
      "    Epoch  8/10: total=1.5549, vae=1.0726, dsb=0.4818, bio=0.0010\n",
      "    Epoch  9/10: total=1.5525, vae=1.0720, dsb=0.4801, bio=0.0009\n",
      "    Epoch 10/10: total=1.5392, vae=1.0720, dsb=0.4668, bio=0.0008\n",
      "\n",
      "======================================================================\n",
      "STAGE 5/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=25.3553, vae=1.0730, dsb=24.2810, bio=0.0024\n",
      "    Epoch  2/10: total=25.1885, vae=1.0722, dsb=24.1158, bio=0.0010\n",
      "    Epoch  3/10: total=25.1467, vae=1.0730, dsb=24.0733, bio=0.0010\n",
      "    Epoch  4/10: total=25.2095, vae=1.0728, dsb=24.1362, bio=0.0010\n",
      "    Epoch  5/10: total=25.1454, vae=1.0721, dsb=24.0729, bio=0.0008\n",
      "    Epoch  6/10: total=25.1666, vae=1.0723, dsb=24.0941, bio=0.0003\n",
      "    Epoch  7/10: total=25.0081, vae=1.0723, dsb=23.9356, bio=0.0004\n",
      "    Epoch  8/10: total=24.8858, vae=1.0715, dsb=23.8137, bio=0.0012\n",
      "    Epoch  9/10: total=24.9759, vae=1.0719, dsb=23.9034, bio=0.0010\n",
      "    Epoch 10/10: total=25.0736, vae=1.0716, dsb=24.0016, bio=0.0009\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=1.6204, vae=1.0741, dsb=0.4346, bio=0.2234\n",
      "    Epoch  2/10: total=1.4991, vae=1.0731, dsb=0.4255, bio=0.0010\n",
      "    Epoch  3/10: total=1.5011, vae=1.0729, dsb=0.4278, bio=0.0007\n",
      "    Epoch  4/10: total=1.5116, vae=1.0735, dsb=0.4376, bio=0.0010\n",
      "    Epoch  5/10: total=1.4952, vae=1.0734, dsb=0.4213, bio=0.0010\n",
      "    Epoch  6/10: total=1.5023, vae=1.0725, dsb=0.4293, bio=0.0010\n",
      "    Epoch  7/10: total=1.4860, vae=1.0727, dsb=0.4129, bio=0.0009\n",
      "    Epoch  8/10: total=1.4916, vae=1.0728, dsb=0.4184, bio=0.0009\n",
      "    Epoch  9/10: total=1.4951, vae=1.0725, dsb=0.4223, bio=0.0005\n",
      "    Epoch 10/10: total=1.4866, vae=1.0719, dsb=0.4143, bio=0.0008\n",
      "\n",
      "[Seed 1] Joint training (original) complete; models and history saved.\n",
      "\n",
      "[Seed 1] Starting joint training (ablation)...\n",
      "\n",
      "======================================================================\n",
      "STAGE 1/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=244.6236, vae=2.0182, dsb=242.6053, bio=0.0000\n",
      "    Epoch  2/10: total=72.1399, vae=1.0738, dsb=71.0660, bio=0.0000\n",
      "    Epoch  3/10: total=63.3894, vae=1.0716, dsb=62.3178, bio=0.0000\n",
      "    Epoch  4/10: total=59.3247, vae=1.0716, dsb=58.2531, bio=0.0000\n",
      "    Epoch  5/10: total=56.8033, vae=1.0706, dsb=55.7328, bio=0.0000\n",
      "    Epoch  6/10: total=55.0039, vae=1.0704, dsb=53.9336, bio=0.0000\n",
      "    Epoch  7/10: total=54.0778, vae=1.0692, dsb=53.0086, bio=0.0000\n",
      "    Epoch  8/10: total=52.8999, vae=1.0706, dsb=51.8293, bio=0.0000\n",
      "    Epoch  9/10: total=52.2905, vae=1.0706, dsb=51.2199, bio=0.0000\n",
      "    Epoch 10/10: total=51.7569, vae=1.0702, dsb=50.6867, bio=0.0000\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=2.9476, vae=1.0705, dsb=1.8771, bio=0.0000\n",
      "    Epoch  2/10: total=2.5040, vae=1.0706, dsb=1.4334, bio=0.0000\n",
      "    Epoch  3/10: total=2.3968, vae=1.0695, dsb=1.3273, bio=0.0000\n",
      "    Epoch  4/10: total=2.3761, vae=1.0707, dsb=1.3054, bio=0.0000\n",
      "    Epoch  5/10: total=2.3119, vae=1.0707, dsb=1.2413, bio=0.0000\n",
      "    Epoch  6/10: total=2.3038, vae=1.0686, dsb=1.2352, bio=0.0000\n",
      "    Epoch  7/10: total=2.2283, vae=1.0704, dsb=1.1579, bio=0.0000\n",
      "    Epoch  8/10: total=2.2210, vae=1.0692, dsb=1.1518, bio=0.0000\n",
      "    Epoch  9/10: total=2.2362, vae=1.0696, dsb=1.1666, bio=0.0000\n",
      "    Epoch 10/10: total=2.2110, vae=1.0690, dsb=1.1420, bio=0.0000\n",
      "\n",
      "======================================================================\n",
      "STAGE 2/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=44.2665, vae=1.0694, dsb=43.1971, bio=0.0000\n",
      "    Epoch  2/10: total=39.9479, vae=1.0704, dsb=38.8775, bio=0.0000\n",
      "    Epoch  3/10: total=38.8738, vae=1.0694, dsb=37.8044, bio=0.0000\n",
      "    Epoch  4/10: total=38.4261, vae=1.0701, dsb=37.3560, bio=0.0000\n",
      "    Epoch  5/10: total=38.0555, vae=1.0697, dsb=36.9858, bio=0.0000\n",
      "    Epoch  6/10: total=37.7946, vae=1.0690, dsb=36.7256, bio=0.0000\n",
      "    Epoch  7/10: total=37.6542, vae=1.0692, dsb=36.5850, bio=0.0000\n",
      "    Epoch  8/10: total=37.5608, vae=1.0702, dsb=36.4906, bio=0.0000\n",
      "    Epoch  9/10: total=37.1181, vae=1.0691, dsb=36.0489, bio=0.0000\n",
      "    Epoch 10/10: total=37.0224, vae=1.0695, dsb=35.9529, bio=0.0000\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=1.9770, vae=1.0698, dsb=0.9072, bio=0.0000\n",
      "    Epoch  2/10: total=1.8938, vae=1.0700, dsb=0.8237, bio=0.0000\n",
      "    Epoch  3/10: total=1.8521, vae=1.0694, dsb=0.7827, bio=0.0000\n",
      "    Epoch  4/10: total=1.8650, vae=1.0698, dsb=0.7952, bio=0.0000\n",
      "    Epoch  5/10: total=1.8250, vae=1.0690, dsb=0.7560, bio=0.0000\n",
      "    Epoch  6/10: total=1.8203, vae=1.0697, dsb=0.7506, bio=0.0000\n",
      "    Epoch  7/10: total=1.8112, vae=1.0695, dsb=0.7417, bio=0.0000\n",
      "    Epoch  8/10: total=1.8283, vae=1.0698, dsb=0.7585, bio=0.0000\n",
      "    Epoch  9/10: total=1.7785, vae=1.0696, dsb=0.7089, bio=0.0000\n",
      "    Epoch 10/10: total=1.7789, vae=1.0696, dsb=0.7094, bio=0.0000\n",
      "\n",
      "======================================================================\n",
      "STAGE 3/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=36.9970, vae=1.0698, dsb=35.9272, bio=0.0000\n",
      "    Epoch  2/10: total=35.4187, vae=1.0691, dsb=34.3496, bio=0.0000\n",
      "    Epoch  3/10: total=35.0105, vae=1.0698, dsb=33.9407, bio=0.0000\n",
      "    Epoch  4/10: total=34.8455, vae=1.0694, dsb=33.7761, bio=0.0000\n",
      "    Epoch  5/10: total=34.8284, vae=1.0695, dsb=33.7588, bio=0.0000\n",
      "    Epoch  6/10: total=34.6528, vae=1.0690, dsb=33.5838, bio=0.0000\n",
      "    Epoch  7/10: total=34.5700, vae=1.0695, dsb=33.5005, bio=0.0000\n",
      "    Epoch  8/10: total=34.4257, vae=1.0697, dsb=33.3560, bio=0.0000\n",
      "    Epoch  9/10: total=34.4603, vae=1.0694, dsb=33.3910, bio=0.0000\n",
      "    Epoch 10/10: total=34.1572, vae=1.0691, dsb=33.0881, bio=0.0000\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=1.7075, vae=1.0689, dsb=0.6386, bio=0.0000\n",
      "    Epoch  2/10: total=1.6480, vae=1.0690, dsb=0.5790, bio=0.0000\n",
      "    Epoch  3/10: total=1.6414, vae=1.0689, dsb=0.5726, bio=0.0000\n",
      "    Epoch  4/10: total=1.6104, vae=1.0695, dsb=0.5410, bio=0.0000\n",
      "    Epoch  5/10: total=1.6205, vae=1.0700, dsb=0.5505, bio=0.0000\n",
      "    Epoch  6/10: total=1.6215, vae=1.0684, dsb=0.5531, bio=0.0000\n",
      "    Epoch  7/10: total=1.6197, vae=1.0685, dsb=0.5511, bio=0.0000\n",
      "    Epoch  8/10: total=1.6139, vae=1.0688, dsb=0.5452, bio=0.0000\n",
      "    Epoch  9/10: total=1.6148, vae=1.0692, dsb=0.5456, bio=0.0000\n",
      "    Epoch 10/10: total=1.6033, vae=1.0687, dsb=0.5346, bio=0.0000\n",
      "\n",
      "======================================================================\n",
      "STAGE 4/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=28.8504, vae=1.0698, dsb=27.7806, bio=0.0000\n",
      "    Epoch  2/10: total=28.1441, vae=1.0691, dsb=27.0750, bio=0.0000\n",
      "    Epoch  3/10: total=27.9883, vae=1.0700, dsb=26.9183, bio=0.0000\n",
      "    Epoch  4/10: total=27.9190, vae=1.0692, dsb=26.8497, bio=0.0000\n",
      "    Epoch  5/10: total=27.9401, vae=1.0686, dsb=26.8715, bio=0.0000\n",
      "    Epoch  6/10: total=28.0041, vae=1.0699, dsb=26.9342, bio=0.0000\n",
      "    Epoch  7/10: total=27.9066, vae=1.0695, dsb=26.8371, bio=0.0000\n",
      "    Epoch  8/10: total=27.7665, vae=1.0705, dsb=26.6960, bio=0.0000\n",
      "    Epoch  9/10: total=27.8407, vae=1.0693, dsb=26.7714, bio=0.0000\n",
      "    Epoch 10/10: total=27.9580, vae=1.0695, dsb=26.8886, bio=0.0000\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=1.5373, vae=1.0695, dsb=0.4678, bio=0.0000\n",
      "    Epoch  2/10: total=1.5197, vae=1.0698, dsb=0.4500, bio=0.0000\n",
      "    Epoch  3/10: total=1.5090, vae=1.0696, dsb=0.4394, bio=0.0000\n",
      "    Epoch  4/10: total=1.5063, vae=1.0694, dsb=0.4370, bio=0.0000\n",
      "    Epoch  5/10: total=1.5011, vae=1.0696, dsb=0.4315, bio=0.0000\n",
      "    Epoch  6/10: total=1.4982, vae=1.0690, dsb=0.4292, bio=0.0000\n",
      "    Epoch  7/10: total=1.4957, vae=1.0698, dsb=0.4260, bio=0.0000\n",
      "    Epoch  8/10: total=1.5085, vae=1.0690, dsb=0.4396, bio=0.0000\n",
      "    Epoch  9/10: total=1.5067, vae=1.0684, dsb=0.4383, bio=0.0000\n",
      "    Epoch 10/10: total=1.4941, vae=1.0685, dsb=0.4256, bio=0.0000\n",
      "\n",
      "======================================================================\n",
      "STAGE 5/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=24.3570, vae=1.0692, dsb=23.2878, bio=0.0000\n",
      "    Epoch  2/10: total=24.2254, vae=1.0687, dsb=23.1567, bio=0.0000\n",
      "    Epoch  3/10: total=24.1821, vae=1.0697, dsb=23.1123, bio=0.0000\n",
      "    Epoch  4/10: total=24.2435, vae=1.0698, dsb=23.1737, bio=0.0000\n",
      "    Epoch  5/10: total=24.1766, vae=1.0695, dsb=23.1071, bio=0.0000\n",
      "    Epoch  6/10: total=24.2114, vae=1.0698, dsb=23.1416, bio=0.0000\n",
      "    Epoch  7/10: total=24.0841, vae=1.0699, dsb=23.0142, bio=0.0000\n",
      "    Epoch  8/10: total=23.9415, vae=1.0693, dsb=22.8722, bio=0.0000\n",
      "    Epoch  9/10: total=24.0349, vae=1.0697, dsb=22.9652, bio=0.0000\n",
      "    Epoch 10/10: total=24.1176, vae=1.0694, dsb=23.0482, bio=0.0000\n",
      "\n",
      "  [Backward Policy Training]\n",
      "    Epoch  1/10: total=1.4641, vae=1.0698, dsb=0.3942, bio=0.0000\n",
      "    Epoch  2/10: total=1.4561, vae=1.0693, dsb=0.3868, bio=0.0000\n",
      "    Epoch  3/10: total=1.4587, vae=1.0693, dsb=0.3894, bio=0.0000\n",
      "    Epoch  4/10: total=1.4688, vae=1.0700, dsb=0.3988, bio=0.0000\n",
      "    Epoch  5/10: total=1.4538, vae=1.0701, dsb=0.3837, bio=0.0000\n",
      "    Epoch  6/10: total=1.4603, vae=1.0692, dsb=0.3911, bio=0.0000\n",
      "    Epoch  7/10: total=1.4450, vae=1.0694, dsb=0.3756, bio=0.0000\n",
      "    Epoch  8/10: total=1.4504, vae=1.0696, dsb=0.3808, bio=0.0000\n",
      "    Epoch  9/10: total=1.4539, vae=1.0694, dsb=0.3844, bio=0.0000\n",
      "    Epoch 10/10: total=1.4456, vae=1.0689, dsb=0.3767, bio=0.0000\n",
      "\n",
      "[Seed 1] Joint training (ablation) complete; models and history saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4191524/1548931930.py:207: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  pre_cells_expr = expression_data[pre_treatment_mask].to(device)\n",
      "/tmp/ipykernel_4191524/1548931930.py:208: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  post_cells_expr = expression_data[post_treatment_mask].to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Seed 1] Drift genes saved in /home/yyuan/ICB_TCE/iter_results/run_001\n",
      "\n",
      "[Seed 1] run finished.\n",
      "\n",
      "================================================================================\n",
      "[Seed 2] Starting running whole pipeline...\n",
      "================================================================================\n",
      "\n",
      "[Seed 2] Starting VAE pre-training...\n",
      "\n",
      "    Epoch [01/20], Beta: 0.000, Average Loss: 535.6858\n",
      "    Epoch [02/20], Beta: 0.000, Average Loss: 305.4417\n",
      "    Epoch [03/20], Beta: 0.000, Average Loss: 267.0555\n",
      "    Epoch [04/20], Beta: 0.000, Average Loss: 248.4453\n",
      "    Epoch [05/20], Beta: 0.100, Average Loss: 245.6155\n",
      "    Epoch [06/20], Beta: 0.200, Average Loss: 244.7749\n",
      "    Epoch [07/20], Beta: 0.300, Average Loss: 244.9857\n",
      "    Epoch [08/20], Beta: 0.400, Average Loss: 245.6118\n",
      "    Epoch [09/20], Beta: 0.500, Average Loss: 246.6355\n",
      "    Epoch [10/20], Beta: 0.600, Average Loss: 247.8422\n",
      "    Epoch [11/20], Beta: 0.700, Average Loss: 249.1421\n",
      "    Epoch [12/20], Beta: 0.800, Average Loss: 250.5059\n",
      "    Epoch [13/20], Beta: 0.900, Average Loss: 251.8101\n",
      "    Epoch [14/20], Beta: 1.000, Average Loss: 253.4411\n",
      "    Epoch [15/20], Beta: 1.000, Average Loss: 253.0251\n",
      "    Epoch [16/20], Beta: 1.000, Average Loss: 252.6597\n",
      "    Epoch [17/20], Beta: 1.000, Average Loss: 252.3067\n",
      "    Epoch [18/20], Beta: 1.000, Average Loss: 251.9870\n",
      "    Epoch [19/20], Beta: 1.000, Average Loss: 251.6090\n",
      "    Epoch [20/20], Beta: 1.000, Average Loss: 251.4167\n",
      "\n",
      "[Seed 2] VAE pre-training complete.\n",
      "\n",
      "[Seed 2] Starting joint training (original)...\n",
      "\n",
      "======================================================================\n",
      "STAGE 1/5\n",
      "======================================================================\n",
      "\n",
      "  [Forward Policy Training]\n",
      "    Epoch  1/10: total=375.4593, vae=3.4948, dsb=359.9087, bio=24.1117\n",
      "    Epoch  2/10: total=104.9745, vae=1.1033, dsb=103.8276, bio=0.0871\n",
      "    Epoch  3/10: total=90.8043, vae=1.0971, dsb=89.6905, bio=0.0332\n",
      "    Epoch  4/10: total=84.2239, vae=1.0953, dsb=83.1190, bio=0.0192\n",
      "    Epoch  5/10: total=80.5547, vae=1.0940, dsb=79.4543, bio=0.0127\n",
      "    Epoch  6/10: total=77.6845, vae=1.0928, dsb=76.5871, bio=0.0093\n",
      "    Epoch  7/10: total=75.4952, vae=1.0929, dsb=74.3986, bio=0.0074\n",
      "    Epoch  8/10: total=73.9958, vae=1.0930, dsb=72.8999, bio=0.0059\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m SEEDS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_SEEDS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m SEEDS:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mrun_single_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_abl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_abl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 112\u001b[0m, in \u001b[0;36mrun_single_seed\u001b[0;34m(seed, config, config_abl)\u001b[0m\n\u001b[1;32m    108\u001b[0m optimizer_vae \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(vae\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_vae\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m))\n\u001b[1;32m    110\u001b[0m vae_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m z: vae\u001b[38;5;241m.\u001b[39mdecoder_output(vae\u001b[38;5;241m.\u001b[39mdecoder(z))\n\u001b[0;32m--> 112\u001b[0m training_history \u001b[38;5;241m=\u001b[39m \u001b[43mrun_joint_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdyn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdyn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_b\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_vae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_vae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpression_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpression_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrn_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrn_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeath_gene_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeath_gene_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbirth_gene_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbirth_gene_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Save original models + history for this run\u001b[39;00m\n\u001b[1;32m    131\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(vae\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae_original_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/ICB_TCE/scripts/joint_train.py:260\u001b[0m, in \u001b[0;36mrun_joint_training_loop\u001b[0;34m(config, dyn, ts, vae, vae_decoder, z_f, z_b, optimizer_f, optimizer_b, optimizer_vae, expression_data, grn_data, death_gene_indices, birth_gene_indices, device)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m \u001b[43mtrain_one_direction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdyn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdyn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae_decoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mother_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_vae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_vae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpression_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpression_data_dev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrn_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrn_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeath_gene_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbirth_gene_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbirth_gene_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m    279\u001b[0m train_one_direction(\n\u001b[1;32m    280\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    281\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     history\u001b[38;5;241m=\u001b[39mtraining_history,\n\u001b[1;32m    295\u001b[0m )\n",
      "File \u001b[0;32m~/ICB_TCE/scripts/joint_train.py:199\u001b[0m, in \u001b[0;36mtrain_one_direction\u001b[0;34m(direction, config, dyn, ts, vae, vae_decoder, z_policy, other_policy, optimizer_policy, optimizer_vae, expression_data, grn_data, death_gene_indices, birth_gene_indices, history)\u001b[0m\n\u001b[1;32m    196\u001b[0m optimizer_policy\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    197\u001b[0m optimizer_vae\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 199\u001b[0m epoch_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_comps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m epoch_loss_vae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_comps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    201\u001b[0m epoch_loss_dsb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_comps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdsb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over 100 seeds and run full pipeline\n",
    "NUM_SEEDS = 100\n",
    "SEEDS = list(range(1, NUM_SEEDS + 1))\n",
    "\n",
    "for s in SEEDS:\n",
    "    run_single_seed(seed=s, config=config, config_abl=config_abl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5892812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 100\n",
    "\n",
    "def aggregate_drift_counts(filename: str, top_k: int = TOP_K, gene_col: str = \"gene\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scan iter_results/run_*/<filename>, collect top-K genes from each file,\n",
    "    and return a DataFrame with total counts per gene.\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "\n",
    "    for run_dir in sorted(glob.glob(os.path.join(iter_dir, \"run_*\"))):\n",
    "        path = os.path.join(run_dir, filename)\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        # Choose gene column: prefer gene_col, otherwise first column\n",
    "        col = gene_col if gene_col in df.columns else df.columns[0]\n",
    "\n",
    "        top_genes = df[col].head(top_k)\n",
    "        counts.update(top_genes)\n",
    "\n",
    "    summary_df = (\n",
    "        pd.DataFrame({\"gene\": list(counts.keys()), \"count\": list(counts.values())})\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    return summary_df\n",
    "\n",
    "# Build summaries for all four drift tables\n",
    "summary_fwd_orig = aggregate_drift_counts(\"drift_genes_forward_original.csv\")\n",
    "summary_bwd_orig = aggregate_drift_counts(\"drift_genes_backward_original.csv\")\n",
    "summary_fwd_abl  = aggregate_drift_counts(\"drift_genes_forward_ablation.csv\")\n",
    "summary_bwd_abl  = aggregate_drift_counts(\"drift_genes_backward_ablation.csv\")\n",
    "\n",
    "# Save to iter_results/summaries/\n",
    "summary_fwd_orig.to_csv(os.path.join(summary_dir, \"summary_drift_forward_original.csv\"), index=False)\n",
    "summary_bwd_orig.to_csv(os.path.join(summary_dir, \"summary_drift_backward_original.csv\"), index=False)\n",
    "summary_fwd_abl.to_csv(os.path.join(summary_dir, \"summary_drift_forward_ablation.csv\"), index=False)\n",
    "summary_bwd_abl.to_csv(os.path.join(summary_dir, \"summary_drift_backward_ablation.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "788f1614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig fwd/bwd: common = 16 | fwd-only = 4 | bwd-only = 4\n",
      "Abl fwd/bwd:  common = 19 | fwd-only = 1 | bwd-only = 1\n"
     ]
    }
   ],
   "source": [
    "TOP_N = 20  # for displaying top genes\n",
    "\n",
    "# Reload summaries to be safe (works even after kernel restart)\n",
    "summary_fwd_orig = pd.read_csv(os.path.join(summary_dir, \"summary_drift_forward_original.csv\"))\n",
    "summary_bwd_orig = pd.read_csv(os.path.join(summary_dir, \"summary_drift_backward_original.csv\"))\n",
    "summary_fwd_abl  = pd.read_csv(os.path.join(summary_dir, \"summary_drift_forward_ablation.csv\"))\n",
    "summary_bwd_abl  = pd.read_csv(os.path.join(summary_dir, \"summary_drift_backward_ablation.csv\"))\n",
    "\n",
    "# Add frequency columns: how often each gene appears across NUM_SEEDS runs\n",
    "for df in (summary_fwd_orig, summary_bwd_orig, summary_fwd_abl, summary_bwd_abl):\n",
    "    df[\"freq\"] = (df[\"count\"] / NUM_SEEDS).round(2)\n",
    "\n",
    "# Take top-N by frequency (or count  theyre monotone here)\n",
    "fwd_orig_top = summary_fwd_orig.sort_values(\"freq\", ascending=False).head(TOP_N).copy()\n",
    "bwd_orig_top = summary_bwd_orig.sort_values(\"freq\", ascending=False).head(TOP_N).copy()\n",
    "fwd_abl_top  = summary_fwd_abl.sort_values(\"freq\",  ascending=False).head(TOP_N).copy()\n",
    "bwd_abl_top  = summary_bwd_abl.sort_values(\"freq\",  ascending=False).head(TOP_N).copy()\n",
    "\n",
    "# Within each model: common fwd vs bwd (your preferred logic)\n",
    "\n",
    "# Original model: forward vs backward\n",
    "orig_common_fb = fwd_orig_top.merge(\n",
    "    bwd_orig_top, on=\"gene\", how=\"inner\", suffixes=(\"_fwd\", \"_bwd\")\n",
    ")\n",
    "orig_fwd_only = fwd_orig_top[~fwd_orig_top[\"gene\"].isin(bwd_orig_top[\"gene\"])]\n",
    "orig_bwd_only = bwd_orig_top[~bwd_orig_top[\"gene\"].isin(fwd_orig_top[\"gene\"])]\n",
    "\n",
    "# Ablation model: forward vs backward\n",
    "abl_common_fb = fwd_abl_top.merge(\n",
    "    bwd_abl_top, on=\"gene\", how=\"inner\", suffixes=(\"_fwd\", \"_bwd\")\n",
    ")\n",
    "abl_fwd_only = fwd_abl_top[~fwd_abl_top[\"gene\"].isin(bwd_abl_top[\"gene\"])]\n",
    "abl_bwd_only = bwd_abl_top[~bwd_abl_top[\"gene\"].isin(fwd_abl_top[\"gene\"])]\n",
    "\n",
    "# Across models: original vs ablation for same direction\n",
    "\n",
    "# Forward: original vs ablation (all genes, outer join)\n",
    "fwd_models_genes = (\n",
    "    summary_fwd_orig[[\"gene\", \"freq\"]].rename(columns={\"freq\": \"freq_orig\"})\n",
    "    .merge(\n",
    "        summary_fwd_abl[[\"gene\", \"freq\"]].rename(columns={\"freq\": \"freq_abl\"}),\n",
    "        on=\"gene\",\n",
    "        how=\"outer\",\n",
    "    )\n",
    "    .fillna(0.0)\n",
    ").round(2)\n",
    "fwd_models_genes[\"freq_diff\"] = fwd_models_genes[\"freq_orig\"] - fwd_models_genes[\"freq_abl\"]\n",
    "\n",
    "# Backward: original vs ablation\n",
    "bwd_models_genes = (\n",
    "    summary_bwd_orig[[\"gene\", \"freq\"]].rename(columns={\"freq\": \"freq_orig\"})\n",
    "    .merge(\n",
    "        summary_bwd_abl[[\"gene\", \"freq\"]].rename(columns={\"freq\": \"freq_abl\"}),\n",
    "        on=\"gene\",\n",
    "        how=\"outer\",\n",
    "    )\n",
    "    .fillna(0.0)\n",
    ").round(2)\n",
    "bwd_models_genes[\"freq_diff\"] = bwd_models_genes[\"freq_orig\"] - bwd_models_genes[\"freq_abl\"]\n",
    "\n",
    "print(\"Orig fwd/bwd: common =\", len(orig_common_fb),\n",
    "      \"| fwd-only =\", len(orig_fwd_only),\n",
    "      \"| bwd-only =\", len(orig_bwd_only))\n",
    "print(\"Abl fwd/bwd:  common =\", len(abl_common_fb),\n",
    "      \"| fwd-only =\", len(abl_fwd_only),\n",
    "      \"| bwd-only =\", len(abl_bwd_only))\n",
    "\n",
    "# Save per-gene forward/backward frequency comparisons\n",
    "fwd_models_genes.to_csv(\n",
    "    os.path.join(summary_dir, \"summary_drift_forward_org_vs_abl.csv\"),\n",
    "    index=False,\n",
    "    float_format=\"%.2f\",\n",
    ")\n",
    "bwd_models_genes.to_csv(\n",
    "    os.path.join(summary_dir, \"summary_drift_backward_org_vs_abl.csv\"),\n",
    "    index=False,\n",
    "    float_format=\"%.2f\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giudsb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
